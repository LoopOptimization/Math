module;

#include "Macros.hxx"
#include "Owner.hxx"
export module ManagedArray;
export import Array;
import Allocator;
import ArrayConcepts;
import ArrayPrint;
import AxisTypes;
import MatDim;
import std;
import Storage;
import Tuple;
import BaseUtils;
export namespace math {
#if __has_feature(address_sanitizer) || defined(__SANITIZE_ADDRESS__)
template <typename T>
using DefaultAlloc = std::allocator<utils::compressed_t<T>>;
#else
template <typename T>
using DefaultAlloc = alloc::Mallocator<utils::compressed_t<T>>;
#endif

/// Stores memory, then pointer.
/// Thus struct's alignment determines initial alignment
/// of the stack memory.
/// Information related to size is then grouped next to the pointer.
///
/// The Intel compiler + OpenMP appears to memcpy data around,
/// or at least build ManagedArrays bypassing the constructors listed here.
/// This caused invalid frees, as the pointer still pointed to the old
/// stack memory.
template <class T, Dimension S,
          std::ptrdiff_t StackStorage = containers::PreAllocStorage<T, S>(),
          alloc::FreeAllocator A = DefaultAlloc<T>>
struct MATH_GSL_OWNER ManagedArray : ResizeableView<T, S> {
  // static_assert(std::is_trivially_destructible_v<T>);
  using BaseT = ResizeableView<T, S>;
  using U = containers::default_capacity_type_t<S>;
  using storage_type = typename BaseT::storage_type;
  static constexpr bool trivialelt =
    std::is_trivially_default_constructible_v<T> &&
    std::is_trivially_move_constructible_v<T> &&
    std::is_trivially_destructible_v<T>;
  // We're deliberately not initializing storage.
#if !defined(__clang__) && defined(__GNUC__)
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
#pragma GCC diagnostic ignored "-Wuninitialized"
#else
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wuninitialized"
#endif
  constexpr ManagedArray() noexcept : BaseT{S{}, U::construct(StackStorage)} {
    this->ptr_ = memory_.data();
#ifndef NDEBUG
    if (!StackStorage) return;
    if constexpr (std::numeric_limits<T>::has_signaling_NaN)
      std::fill_n(this->data(), StackStorage,
                  std::numeric_limits<T>::signaling_NaN());
    else if constexpr (std::numeric_limits<T>::is_specialized)
      std::fill_n(this->data(), StackStorage, std::numeric_limits<T>::min());
#endif
  }
  // if `T` is trivial, contents are uninitialized
  // if non-trivial, they are default constructed.
  constexpr ManagedArray(S s) noexcept
    : BaseT{s, U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    U len = U(capacity(std::ptrdiff_t(this->sz_)));
    if (std::ptrdiff_t(len) > StackStorage) this->allocateAtLeast(len);
#ifndef NDEBUG
    if (!len) return;
    auto l = std::ptrdiff_t(len);
    if constexpr (std::numeric_limits<T>::has_signaling_NaN)
      std::fill_n(this->data(), l, std::numeric_limits<T>::signaling_NaN());
    else if constexpr (std::numeric_limits<T>::is_specialized)
      std::fill_n(this->data(), l, std::numeric_limits<T>::min());
#endif
    if constexpr (!trivialelt)
      std::uninitialized_default_construct_n(this->data(), len);
  }
  constexpr ManagedArray(S s, T x) noexcept
    : BaseT{s, U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(this->sz_);
    if (len > StackStorage) this->allocateAtLeast(U(capacity(len)));
    if (!len) return;
    if constexpr (trivialelt) std::fill_n(this->data(), len, x);
    else std::uninitialized_fill_n(this->data(), len, std::move(x));
  }
  TRIVIAL constexpr ManagedArray(A) noexcept : ManagedArray(){};
  constexpr ManagedArray(S s, A) noexcept : ManagedArray(s) {};
  constexpr ManagedArray(std::ptrdiff_t s, A) noexcept
    requires(std::same_as<S, SquareDims<>>)
    : ManagedArray(SquareDims<>{row(s)}) {};
  constexpr ManagedArray(S s, T x, A) noexcept : ManagedArray(s, x) {};

  constexpr ManagedArray(T x) noexcept requires(RowVectorDimension<S>)
    : BaseT{S{}, U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    if constexpr (StackStorage == 0) this->growUndef(1);
    this->push_back_within_capacity(std::move(x));
  }

  template <class D>
  constexpr ManagedArray(const ManagedArray<T, D, StackStorage, A> &b) noexcept
    : BaseT{S(b.dim()), U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(this->sz_);
    this->growUndef(len);
    if constexpr (trivialelt) std::copy_n(b.data(), len, this->data());
    else std::uninitialized_copy_n(b.data(), len, this->data());
  }
  template <std::convertible_to<T> Y, class D, class AY>
  constexpr ManagedArray(const ManagedArray<Y, D, StackStorage, AY> &b) noexcept
    : BaseT{S{}, U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    S d = b.dim();
    auto len = std::ptrdiff_t(d);
    this->growUndef(len);
    this->sz_ = d;
    if constexpr (trivialelt) std::copy_n(b.data(), len, this->data());
    else std::uninitialized_copy_n(b.data(), len, this->data());
  }
  template <std::convertible_to<T> Y, std::size_t M>
  constexpr ManagedArray(std::array<Y, M> il) noexcept
    : BaseT{S{}, U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(M);
    this->growUndef(len);
    if constexpr (trivialelt) std::copy_n(il.begin(), len, this->data());
    else std::uninitialized_copy_n(il.begin(), len, this->data());
    this->sz_ = math::length(std::ptrdiff_t(M));
  }
  template <std::convertible_to<T> Y, class D, class AY>
  constexpr ManagedArray(const ManagedArray<Y, D, StackStorage, AY> &b,
                         S s) noexcept
    : BaseT{S(s), U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(this->sz_);
    invariant(len == U(b.size()));
    this->growUndef(len);
    T *p = this->data();
    if constexpr (trivialelt) std::copy_n(b.data(), len, this->data());
    else std::uninitialized_copy_n(b.data(), len, this->data());
  }
  constexpr ManagedArray(const ManagedArray &b) noexcept
    : BaseT{S(b.dim()), U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(this->sz_);
    this->growUndef(len);
    if constexpr (trivialelt) std::copy_n(b.data(), len, this->data());
    else std::uninitialized_copy_n(b.data(), len, this->data());
  }
  constexpr ManagedArray(const Array<T, S> &b) noexcept
    : BaseT{S(b.dim()), U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    auto len = std::ptrdiff_t(this->sz_);
    this->growUndef(len);
    if constexpr (trivialelt) std::copy_n(b.data(), len, this->data());
    else std::uninitialized_copy_n(b.data(), len, this->data());
  }
  template <AbstractSimilar<S> V>
  constexpr ManagedArray(const V &b) noexcept
    : BaseT{S(shape(b)), U(math::Capacity<StackStorage, std::ptrdiff_t>{})} {
    this->ptr_ = memory_.data();
    this->growUndef(std::ptrdiff_t(this->sz_));
    (*this) << b;
  }
  template <class D>
  constexpr ManagedArray(ManagedArray<T, D, StackStorage, A> &&b) noexcept
    : BaseT{b.dim(), U(capacity(StackStorage))} {
    if (!b.isSmall()) { // steal
      this->ptr_ = b.data();
      this->capacity_ = b.getCapacity();
    } else {
      this->ptr_ = memory_.data();
      if constexpr (trivialelt)
        std::copy_n(b.data(), std::ptrdiff_t(b.dim()), this->data());
      else
        std::uninitialized_copy_n(b.data(), std::ptrdiff_t(b.dim()),
                                  this->data());
    }
    b.resetNoFree();
  }
  constexpr ManagedArray(ManagedArray &&b) noexcept
    : BaseT{b.dim(), U(capacity(StackStorage))} {
    if constexpr (StackStorage) {
      if (!b.isSmall()) { // steal
        this->ptr_ = b.data();
        this->capacity_ = b.getCapacity();
      } else {
        this->ptr_ = memory_.data();
        if constexpr (trivialelt)
          std::copy_n(b.data(), std::ptrdiff_t(b.dim()), this->data());
        else
          std::uninitialized_copy_n(b.data(), std::ptrdiff_t(b.dim()),
                                    this->data());
      }
    } else {
      this->ptr_ = b.ptr_;
      this->capacity_ = b.getCapacity();
    }
    b.resetNoFree();
  }
  template <class D>
  constexpr ManagedArray(ManagedArray<T, D, StackStorage, A> &&b, S s) noexcept
    : BaseT{s, U(capacity(StackStorage))} {
    if (!b.isSmall()) { // steal
      this->ptr_ = b.data();
      this->capacity_ = b.getCapacity();
    } else {
      this->ptr_ = memory_.data();
      if constexpr (trivialelt)
        std::copy_n(b.data(), std::ptrdiff_t(b.dim()), this->data());
      else
        std::uninitialized_copy_n(b.data(), std::ptrdiff_t(b.dim()),
                                  this->data());
    }
    b.resetNoFree();
  }
  constexpr ManagedArray(const ColVector auto &v) requires(MatrixDimension<S>)
    : BaseT{S(shape(v)), U(capacity(StackStorage))} {
    this->ptr_ = memory_.data();
    std::ptrdiff_t M = std::ptrdiff_t(this->sz_);
    this->growUndef(M);
    if constexpr (!trivialelt)
      std::uninitialized_default_construct_n(this->data(), M);
    MutArray<T, decltype(v.dim())>(this->data(), v.dim()) << v;
  }
  constexpr ManagedArray(const RowVector auto &v) requires(MatrixDimension<S>)
    : BaseT{S(CartesianIndex(1, v.size())), U(capacity(StackStorage))} {
    this->ptr_ = memory_.data();
    std::ptrdiff_t M = std::ptrdiff_t(this->sz_);
    this->growUndef(M);
    if constexpr (!trivialelt)
      std::uninitialized_default_construct_n(this->data(), M);
    MutArray<T, decltype(v.dim())>(this->data(), v.dim()) << v;
  }
#if !defined(__clang__) && defined(__GNUC__)
#pragma GCC diagnostic pop
#pragma GCC diagnostic pop
#else
#pragma clang diagnostic pop
#endif

  template <class D>
  constexpr auto
  operator=(const ManagedArray<T, D, StackStorage, A> &b) noexcept
    -> ManagedArray & requires(!std::same_as<S, D>) {
    // this condition implies `this->data() == nullptr`
    if (this->data() == b.data()) return *this;
    resizeCopyTo(b);
    return *this;
  }
  template <class D>
  constexpr auto operator=(ManagedArray<T, D, StackStorage, A> &&b) noexcept
    -> ManagedArray & requires(!std::same_as<S, D>) {
    // this condition implies `this->data() == nullptr`
    if (this->data() == b.data()) return *this;
    // here, we commandeer `b`'s memory
    S d = b.dim();
    // if `b` is small, we need to copy memory
    // no need to shrink our capacity
    if (b.isSmall()) std::copy_n(b.data(), std::ptrdiff_t(d), this->data());
    else this->maybeDeallocate(b.data(), std::ptrdiff_t(b.getCapacity()));
    b.resetNoFree();
    this->sz_ = d;
    return *this;
  }
  constexpr auto operator=(const ManagedArray &b) noexcept -> ManagedArray & {
    if (this == &b) return *this;
    resizeCopyTo(b);
    return *this;
  }
  constexpr auto operator=(ManagedArray &&b) noexcept -> ManagedArray & {
    if (this == &b) return *this;
    // here, we commandeer `b`'s memory
    S d = b.dim();
    if (b.isSmall()) std::copy_n(b.data(), std::ptrdiff_t(d), this->data());
    else this->maybeDeallocate(b.data(), std::ptrdiff_t(b.getCapacity()));
    b.resetNoFree();
    this->sz_ = d;
    return *this;
  }
  constexpr void resetNoFree() {
    this->ptr_ = memory_.data();
    this->sz_ = S{};
    this->capacity_ = U(math::Capacity<StackStorage, std::ptrdiff_t>{});
  }
  TRIVIAL constexpr ~ManagedArray() noexcept { this->maybeDeallocate(); }

  [[nodiscard]] TRIVIAL static constexpr auto identity(std::ptrdiff_t M)
    -> ManagedArray requires(MatrixDimension<S>) {
    ManagedArray B(SquareDims<>{row(M)}, T{0});
    B.diag() << 1;
    return B;
  }
  [[nodiscard]] static constexpr auto identity(Row<> R) -> ManagedArray
    requires(MatrixDimension<S>) {
    return identity(std::ptrdiff_t(R));
  }
  [[nodiscard]] static constexpr auto identity(Col<> C) -> ManagedArray
    requires(MatrixDimension<S>) {
    return identity(std::ptrdiff_t(C));
  }

  TRIVIAL constexpr void reserveForGrow1() requires(RowVectorDimension<S>) {
    auto s = std::ptrdiff_t(this->sz_), c = std::ptrdiff_t(this->capacity_);
    if (s == c) [[unlikely]]
      reserve(length(newCapacity(c)));
  }

  template <class... Args>
  TRIVIAL constexpr auto emplace_back(Args &&...args) -> decltype(auto)
    requires(RowVectorDimension<S>) {
    reserveForGrow1();
    return this->emplace_back_within_capacity(args...);
  }
  TRIVIAL constexpr void push_back(T value) requires(RowVectorDimension<S>) {
    reserveForGrow1();
    this->push_back_within_capacity(std::move(value));
  }
  constexpr auto insert(T *p, T x) -> T * requires(RowVectorDimension<S>) {
    auto s = std::ptrdiff_t(this->sz_), c = std::ptrdiff_t(this->capacity_);
    if (s == c) [[unlikely]] {
      std::ptrdiff_t d = p - this->data();
      reserve(length(newCapacity(c)));
      p = this->data() + d;
    }
    invariant(s == std::ptrdiff_t(this->sz_));
    return this->insert_within_capacity(p, std::move(x));
  }
  // behavior
  // if S is StridedDims, then we copy data.
  // If the new dims are larger in rows or cols, we fill with 0.
  // If the new dims are smaller in rows or cols, we truncate.
  // New memory outside of dims (i.e., stride larger), we leave uninitialized.
  //
  TRIVIAL constexpr void resize(S nz) {
    reallocForSize(nz);
    this->sz_ = nz;
  }
  TRIVIAL constexpr void resize(Row<> r)
    requires(RowVectorDimension<S> || MatrixDimension<S>) {
    if constexpr (RowVectorDimension<S>) return resize(S(r));
    else return resize(auto{this->sz_}.set(r));
  }
  TRIVIAL constexpr void resize(Col<> c)
    requires(RowVectorDimension<S> || MatrixDimension<S>) {
    if constexpr (RowVectorDimension<S>) return resize(S(c));
    else if constexpr (MatrixDimension<S>)
      return resize(auto{this->sz_}.set(c));
  }
  constexpr void resizeForOverwrite(S M) {
    auto nz = std::ptrdiff_t(M);
    if constexpr (!trivialelt) {
      std::ptrdiff_t oz = std::ptrdiff_t(this->sz_);
      storage_type *old_ptr = this->data();
      if (nz < oz) std::destroy_n(old_ptr + nz, oz - nz);
      else if (nz > std::ptrdiff_t(this->capacity_)) {
        auto [new_ptr, new_cap] = alloc::alloc_at_least(A{}, nz);
        invariant(std::ptrdiff_t(new_cap) >= nz);
        std::uninitialized_move_n(old_ptr, oz, new_ptr);
        std::uninitialized_default_construct_n(new_ptr + oz, nz - oz);
        maybeDeallocate();
        this->ptr_ = new_ptr;
        this->capacity_ = U::construct(new_cap);
      } else if (nz > oz)
        std::uninitialized_default_construct_n(old_ptr + oz, nz - oz);
    } else if (nz > std::ptrdiff_t(this->sz_)) growUndef(nz);
    this->sz_ = M;
  }
  TRIVIAL constexpr void resizeForOverwrite(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    resizeForOverwrite(length(M));
  }
  TRIVIAL constexpr void resize(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    resize(length(M));
  }
  TRIVIAL constexpr void reserve(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    reserve(length(M));
  }
  TRIVIAL constexpr void reserveGrow(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    if (std::ptrdiff_t c = std::ptrdiff_t(this->getCapacity()); c < M)
      reserve(length(newCapacity(M)));
  }
  /// @brief Resize and grow capacity aggressively.
  TRIVIAL constexpr void resizeGrow(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    if (std::ptrdiff_t c = std::ptrdiff_t(this->getCapacity()); c < M)
      reserve(length(newCapacity(M)));
    resize(length(M));
  }
  TRIVIAL constexpr void reservePow2(std::ptrdiff_t M)
    requires(RowVectorDimension<S>) {
    // 1 -> 0 -> 64 -> 0
    // 2 -> 1 -> 63 -> 1
    // 3 -> 2 -> 62 -> 2
    // 4 -> 3 -> 62 -> 2
    // 5 -> 4 -> 61 -> 3
    // 6 -> 5 -> 61 -> 3
    // 7 -> 6 -> 61 -> 3
    // 8 -> 7 -> 61 -> 3
    // 9 -> 8 -> 60 -> 4
    std::size_t l2m =
      1 << ((8 * sizeof(M)) - std::countl_zero(std::size_t(M) - 1));
    reserve(length(std::ptrdiff_t(l2m)));
  }
  constexpr void resizeForOverwrite(Row<> r) {
    if constexpr (RowVectorDimension<S>) {
      return resizeForOverwrite(S(r));
    } else if constexpr (MatrixDimension<S>) {
      S nz = this->sz_;
      return resizeForOverwrite(nz.set(r));
    }
  }
  constexpr void resizeForOverwrite(Col<> c) {
    if constexpr (RowVectorDimension<S>) {
      return resizeForOverwrite(S(c));
    } else if constexpr (MatrixDimension<S>) {
      S nz = this->sz_;
      return resizeForOverwrite(nz.set(c));
    }
  }
  static constexpr auto reserveCore(S nz, storage_type *op,
                                    std::ptrdiff_t old_len, U oc,
                                    bool was_allocated)
    -> containers::Pair<storage_type *, U> {
    auto new_capacity = std::ptrdiff_t(nz);
    invariant(new_capacity >= 0z);
    if (new_capacity <= oc) return {op, oc};
    // allocate new, copy, deallocate old
    auto [new_ptr, new_cap] = alloc::alloc_at_least(A{}, new_capacity);
    auto nc = std::ptrdiff_t(new_cap);
    invariant(nc >= new_capacity);
    if (old_len) {
      if constexpr (trivialelt)
        std::memcpy(new_ptr, op, old_len * sizeof(storage_type));
      else std::uninitialized_move_n(op, old_len, new_ptr);
    }
    maybeDeallocate(op, old_len, std::ptrdiff_t(oc), was_allocated);
    return {new_ptr, U(capacity(nc))};
  }
  TRIVIAL constexpr void reserve(S nz) {
    auto [np, nc] = reserveCore(nz, this->data(), std::ptrdiff_t(this->sz_),
                                this->capacity_, wasAllocated());
    this->ptr_ = np;
    this->capacity_ = nc;
  }
  [[nodiscard]] TRIVIAL constexpr auto get_allocator() const noexcept -> A {
    return {};
  }
  // set size and 0.
  TRIVIAL constexpr void setSize(Row<> r, Col<> c) {
    resizeForOverwrite({r, c});
    this->fill(0);
  }
  TRIVIAL constexpr void resize(Row<> MM, Col<> NN) {
    resize(DenseDims{MM, NN});
  }
  TRIVIAL constexpr void reserve(Row<> M, Col<> N) {
    if constexpr (std::is_same_v<S, StridedDims<>>)
      reserve(StridedDims{M, N, max(N, RowStride(this->dim()))});
    else if constexpr (std::is_same_v<S, SquareDims<>>)
      reserve(SquareDims{row(std::max(std::ptrdiff_t(M), std::ptrdiff_t(N)))});
    else reserve(DenseDims{M, N});
  }
  TRIVIAL constexpr void reserve(Row<> M, RowStride<> X) {
    if constexpr (std::is_same_v<S, StridedDims<>>)
      reserve(S{M, col(std::ptrdiff_t(X)), X});
    else if constexpr (std::is_same_v<S, SquareDims<>>)
      reserve(SquareDims{row(std::max(std::ptrdiff_t(M), std::ptrdiff_t(X)))});
    else reserve(S{M, col(std::ptrdiff_t(X))});
  }
  TRIVIAL constexpr void clearReserve(Row<> M, Col<> N) {
    this->clear();
    reserve(M, N);
  }
  TRIVIAL constexpr void clearReserve(Row<> M, RowStride<> X) {
    this->clear();
    reserve(M, X);
  }
  TRIVIAL constexpr void resizeForOverwrite(Row<> M, Col<> N, RowStride<> X) {
    invariant(X >= N);
    if constexpr (std::is_same_v<S, StridedDims<>>)
      resizeForOverwrite(S{M, N, X});
    else if constexpr (std::is_same_v<S, SquareDims<>>) {
      invariant(std::ptrdiff_t(M) == std::ptrdiff_t(N));
      resizeForOverwrite(S{M});
    } else resizeForOverwrite(S{M, N});
  }
  TRIVIAL constexpr void resizeForOverwrite(Row<> M, Col<> N) {
    if constexpr (std::is_same_v<S, StridedDims<>>)
      resizeForOverwrite(S{M, N, {std::ptrdiff_t(N)}});
    else if constexpr (std::is_same_v<S, SquareDims<>>) {
      invariant(std::ptrdiff_t(M) == std::ptrdiff_t(N));
      resizeForOverwrite(S{M});
    } else resizeForOverwrite(S{M, N});
  }
  TRIVIAL [[nodiscard]] constexpr auto isSmall() const -> bool {
    invariant(this->capacity_ >= StackStorage);
    return this->capacity_ == StackStorage;
  }

private:
  constexpr void allocateAtLeast(U len) {
    auto l = std::size_t(std::ptrdiff_t(len));
    alloc::AllocResult<storage_type> res = alloc::alloc_at_least(A{}, l);
    this->ptr_ = res.ptr;
    invariant(res.count >= l);
    this->capacity_ = U::construct(res.count);
  }
  TRIVIAL [[nodiscard]] constexpr auto wasAllocated() const -> bool {
    return !isSmall();
  }
  // this method should only be called from the destructor
  // (and the implementation taking the new ptr and capacity)
  void maybeDeallocate() noexcept {
    if constexpr (StackStorage > 0)
      maybeDeallocate(this->data(), std::ptrdiff_t(this->sz_),
                      std::ptrdiff_t(this->capacity_), wasAllocated());
    else
      maybeDeallocate(const_cast<storage_type *>(this->ptr_),
                      std::ptrdiff_t(this->sz_),
                      std::ptrdiff_t(this->capacity_), wasAllocated());
  }
  static void maybeDeallocate(storage_type *p, std::ptrdiff_t sz,
                              std::ptrdiff_t cap, bool was_allocated) noexcept {
    if constexpr (!std::is_trivially_destructible_v<storage_type>)
      std::destroy_n(p, sz);
    if (was_allocated) A{}.deallocate(p, cap);
  }
  // this method should be called whenever the buffer lives
  // NOTE: it is invalid to reassign `sz` before calling `maybeDeallocate`!
  void maybeDeallocate(storage_type *newPtr,
                       std::ptrdiff_t newCapacity) noexcept {
    maybeDeallocate(this->data(), std::ptrdiff_t(this->sz_),
                    std::ptrdiff_t(this->capacity_), wasAllocated());
    this->ptr_ = newPtr;
    this->capacity_ = U(capacity(newCapacity));
  }
  // reallocate, discarding old data
  // This only performs the allocation!!
  void growUndef(std::ptrdiff_t M) {
    invariant(M >= 0);
    if (M <= this->capacity_) return;
    maybeDeallocate();
    // because this doesn't care about the old data,
    // we can allocate after freeing, which may be faster
    this->ptr_ = A{}.allocate(M);
    this->capacity_ = U(capacity(M));
    // if constexpr (!trivialelt)
    //   std::uninitialized_default_construct_n(this->data(), M);
#ifndef NDEBUG
    if constexpr (std::numeric_limits<T>::has_signaling_NaN)
      std::fill_n(this->data(), M, std::numeric_limits<T>::signaling_NaN());
    else if constexpr (std::integral<T>)
      std::fill_n(this->data(), M, std::numeric_limits<T>::min());
#endif
  }
  constexpr void reallocForSize(S nz) {
    S oz = this->sz_;
    if constexpr (RowVectorDimension<S>) {
      auto ozs = std::ptrdiff_t(oz), nzs = std::ptrdiff_t(nz);
      storage_type *old_ptr = this->data();
      if (nz <= oz) {
        if constexpr (!std::is_trivially_destructible_v<T>)
          if (nz < oz) std::destroy_n(old_ptr + nzs, ozs - nzs);
        return;
      }
      if (nz > this->capacity_) {
        auto ncu = std::size_t(nzs);
        auto [new_ptr, new_cap] = alloc::alloc_at_least(A{}, ncu);
        invariant(new_cap >= ncu);
        auto ncs = std::ptrdiff_t(new_cap);
        invariant(ncs >= nzs);
        if constexpr (trivialelt) {
          if (oz) std::copy_n(old_ptr, ozs, new_ptr);
          std::fill(new_ptr + ozs, new_ptr + nzs, T{});
        } else {
          if (oz) std::uninitialized_move_n(old_ptr, ozs, new_ptr);
          std::uninitialized_default_construct(new_ptr + ozs, new_ptr + nzs);
        }
        maybeDeallocate(new_ptr, ncs);
      } else if (nz > oz) {
        if constexpr (trivialelt) std::fill(old_ptr + ozs, old_ptr + nzs, T{});
        else std::uninitialized_default_construct(old_ptr + ozs, old_ptr + nzs);
      }
    } else {
      static_assert(std::is_trivially_destructible_v<T>,
                    "Resizing matrices holding non-is_trivially_destructible_v "
                    "objects is not yet supported.");
      static_assert(MatrixDimension<S>, "Can only resize 1 or 2d containers.");
      auto len = std::ptrdiff_t(nz);
      if (len == 0) return;
      auto new_x = std::ptrdiff_t{RowStride(nz)},
           old_x = std::ptrdiff_t{RowStride(oz)},
           new_n = std::ptrdiff_t{Col(nz)}, old_n = std::ptrdiff_t{Col(oz)},
           new_m = std::ptrdiff_t{Row(nz)}, old_m = std::ptrdiff_t{Row(oz)};
      bool new_alloc = len > this->capacity_;
      bool in_place = !new_alloc;
      T *npt = this->data();
      if (new_alloc) {
        alloc::AllocResult<T> res = alloc::alloc_at_least(A{}, len);
        npt = res.ptr;
        len = res.count;
      }
      // we can copy forward so long as the new stride is smaller
      // so that the start of the dst range is outside of the src range
      // we can also safely forward copy if we allocated a new ptr
      bool forward_copy = (new_x <= old_x) || new_alloc;
      std::ptrdiff_t cols_to_copy = std::min(old_n, new_n);
      // we only need to copy if memory shifts position
      bool copy_cols = new_alloc || ((cols_to_copy > 0) && (new_x != old_x));
      // if we're in place, we have 1 less row to copy
      std::ptrdiff_t rows_to_copy = std::min(old_m, new_m);
      std::ptrdiff_t fill_count = new_n - cols_to_copy;
      if ((rows_to_copy) && (copy_cols || fill_count)) {
        if (forward_copy) {
          // truncation, we need to copy rows to increase stride
          T *src = this->data();
          T *dst = npt;
          do {
            if (copy_cols && (!in_place)) std::copy_n(src, cols_to_copy, dst);
            if (fill_count) std::fill_n(dst + cols_to_copy, fill_count, T{});
            src += old_x;
            dst += new_x;
            in_place = false;
          } while (--rows_to_copy);
        } else /* [[unlikely]] */ {
          // backwards copy, only needed when we increasing stride but not
          // reallocating, which should be comparatively uncommon.
          // Should probably benchmark or determine actual frequency
          // before adding `[[unlikely]]`.
          invariant(in_place);
          T *src = this->data() + ((rows_to_copy + in_place) * old_x);
          T *dst = npt + ((rows_to_copy + in_place) * new_x);
          do {
            src -= old_x;
            dst -= new_x;
            if (cols_to_copy && (rows_to_copy > in_place))
              std::copy_backward(src, src + cols_to_copy, dst + cols_to_copy);
            if (fill_count) std::fill_n(dst + cols_to_copy, fill_count, T{});
          } while (--rows_to_copy);
        }
      }
      // zero init remaining rows
      for (std::ptrdiff_t m = old_m; m < new_m; ++m)
        std::fill_n(npt + (m * new_x), new_n, T{});
      if (new_alloc) maybeDeallocate(npt, len);
    }
  }

  // copies and resizes
  void resizeCopyTo(const auto &b) {
    S d = b.dim();
    auto len = std::ptrdiff_t(d);
    storage_type *old_ptr = this->data();
    const storage_type *bptr = b.data();
    if constexpr (trivialelt) {
      this->growUndef(len);
      std::copy_n(bptr, len, old_ptr);
    } else {
      std::ptrdiff_t oz = std::ptrdiff_t(this->sz_), nz = std::ptrdiff_t(d);
      if (nz > std::ptrdiff_t(this->capacity_)) {
        auto [new_ptr, new_cap] = alloc::alloc_at_least(A{}, nz);
        invariant(new_cap >= nz);
        for (std::ptrdiff_t i = 0; i < oz; ++i)
          *std::construct_at(new_ptr + i, std::move(old_ptr[i])) = bptr[i];
        std::uninitialized_copy_n(bptr + oz, nz - oz, new_ptr);
        maybeDeallocate();
        this->ptr_ = new_ptr;
        this->capacity_ = U(capacity(new_cap));
      } else {
        std::copy_n(bptr, std::min(nz, oz), old_ptr);
        if (nz < oz) std::destroy_n(old_ptr + nz, oz - nz);
        else std::uninitialized_copy_n(bptr + oz, nz - oz, old_ptr);
      }
    }
    this->sz_ = d;
  }

  [[no_unique_address]] containers::Storage<storage_type, StackStorage> memory_;
};

static_assert(std::move_constructible<ManagedArray<std::int64_t, Length<>>>);
static_assert(std::copyable<ManagedArray<std::int64_t, Length<>>>);
// Check that `[[no_unique_address]]` is working.
// sizes should be:
// [ptr, dims, capacity, array]
// 8 + 3*4 + 4 + 0 + 64*8 = 24 + 512 = 536
static_assert(sizeof(ManagedArray<std::int64_t, StridedDims<>, 64,
                                  alloc::Mallocator<std::int64_t>>) == 552);
// sizes should be:
// [ptr, dims, capacity, array]
// 8 + 2*4 + 8 + 0 + 64*8 = 24 + 512 = 536
static_assert(sizeof(ManagedArray<std::int64_t, DenseDims<>, 64,
                                  alloc::Mallocator<std::int64_t>>) == 536);
// sizes should be:
// [ptr, dims, capacity, array]
// 8 + 1*4 + 4 + 0 + 64*8 = 16 + 512 = 528
static_assert(sizeof(ManagedArray<std::int64_t, SquareDims<>, 64,
                                  alloc::Mallocator<std::int64_t>>) == 536);

template <class T,
          std::ptrdiff_t N = containers::PreAllocStorage<T, Length<>>()>
#ifdef __APPLE__
// Apple pads out `Vector` to 24B anyway, so may as well use full-size
using Vector = ManagedArray<T, Length<-1, std::ptrdiff_t>, N>;
#else
using Vector = ManagedArray<T, Length<-1, std::int32_t>, N>;
#endif

template <class T,
          std::ptrdiff_t L = containers::PreAllocStorage<T, StridedDims<>>()>
using Matrix = ManagedArray<T, StridedDims<>, L>;
template <class T,
          std::ptrdiff_t L = containers::PreAllocStorage<T, DenseDims<>>()>
using DenseMatrix = ManagedArray<T, DenseDims<>, L>;
template <class T,
          std::ptrdiff_t L = containers::PreAllocStorage<T, SquareDims<>>()>
using SquareMatrix = ManagedArray<T, SquareDims<>, L>;

// type def defined by allocator
template <alloc::FreeAllocator A,
          std::ptrdiff_t L =
            containers::PreAllocStorage<utils::eltype_t<A>, SquareDims<>>()>
using SquareMatrixAlloc = ManagedArray<utils::eltype_t<A>, SquareDims<>, L, A>;
template <alloc::FreeAllocator A, std::ptrdiff_t R, std::ptrdiff_t C,
          std::ptrdiff_t L =
            containers::PreAllocStorage<utils::eltype_t<A>, DenseDims<>>()>
using DenseMatrixAlloc =
  ManagedArray<utils::eltype_t<A>, DenseDims<R, C>, L, A>;
template <alloc::FreeAllocator A,
          std::ptrdiff_t L =
            containers::PreAllocStorage<utils::eltype_t<A>, StridedDims<>>()>
using StrideMatrixAlloc = ManagedArray<utils::eltype_t<A>, StridedDims<>, L, A>;

template <VectorDimension S = std::ptrdiff_t>
using IntVector = ManagedArray<std::int64_t, S>;
template <MatrixDimension S = DenseDims<>>
using IntMatrix = ManagedArray<std::int64_t, S>;

static_assert(sizeof(ManagedArray<std::int32_t, DenseDims<3, 5>, 15>) ==
              sizeof(std::int32_t *) + 16 * sizeof(std::int32_t));
static_assert(sizeof(ManagedArray<std::int32_t, DenseDims<>, 15>) ==
              sizeof(std::int32_t *) + 2 * sizeof(std::int32_t) +
                sizeof(std::ptrdiff_t) + 16 * sizeof(std::int32_t));
static_assert(AbstractMatrix<ManagedArray<std::int64_t, SquareDims<>>>);
static_assert(
  std::is_convertible_v<DenseMatrix<std::int64_t>, Matrix<std::int64_t>>);
static_assert(std::is_convertible_v<DenseMatrix<std::int64_t>,
                                    DensePtrMatrix<std::int64_t>>);
static_assert(
  std::is_convertible_v<DenseMatrix<std::int64_t>, PtrMatrix<std::int64_t>>);
static_assert(
  std::is_convertible_v<SquareMatrix<std::int64_t>, Matrix<std::int64_t>>);
static_assert(std::is_convertible_v<SquareMatrix<std::int64_t>,
                                    MutPtrMatrix<std::int64_t>>);
static_assert(std::same_as<IntMatrix<>::value_type, std::int64_t>);
static_assert(AbstractMatrix<IntMatrix<>>);
static_assert(std::copyable<IntMatrix<>>);
static_assert(std::move_constructible<Vector<std::int64_t>>);
static_assert(DefinesShape<Vector<std::int64_t>>);
static_assert(RowVectorCore<Vector<std::int64_t>>);
static_assert(std::copyable<Vector<std::int64_t>>);
static_assert(AbstractVector<Vector<std::int64_t>>);
static_assert(!std::is_trivially_copyable_v<Vector<std::int64_t>>);
static_assert(!std::is_trivially_destructible_v<Vector<std::int64_t>>);
static_assert(AbstractVector<Vector<std::int64_t>>,
              "PtrVector<std::int64_t> isa AbstractVector failed");
static_assert(
  std::same_as<utils::eltype_t<Matrix<std::int64_t>>, std::int64_t>);
static_assert(sizeof(Vector<std::int64_t, 0>) == 16);

} // namespace math
